# Summary dataset used

[News Summary](https://www.kaggle.com/edumunozsala/cleaned-news-summary) (.csv)

[AMI corpus](https://github.com/gcunhase/AMICorpusXML) (text and summary)

[WikiHow](https://www.kaggle.com/varunucl/wikihow-summarization) (.csv)

# Data Processing

1. Convert them to (text, summary) format

We would have three directory TRAIN, VALIDATE, TEST 

2. Convert each of directory to a set of bin to pass in the code.

# Summary dataset found

## News

[News Summary](https://www.kaggle.com/sunnysai12345/news-summary) \[Summary to a few words headline. [Extended, cleaned version](https://www.kaggle.com/edumunozsala/cleaned-news-summary) \] -> This is what we picked

[BBC News Summary](https://www.kaggle.com/pariza/bbc-news-summary) \[Summary to about 1/3, very long long long]

## Paper

[scisumm-corpus](https://github.com/WING-NUS/scisumm-corpus)

[TalkSumm](https://github.com/levguy/talksumm)

## Article

[NewsRoom](https://paperswithcode.com/dataset/newsroom) \[large scale\]

## Guideline

[WikiHow](https://www.kaggle.com/varunucl/wikihow-summarization) -> This is what we picked

## Meeting


[AMI corpus](https://github.com/gcunhase/AMICorpusXML) \[The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings.\] -> This is what we picked

## Legal case report

[Legal Case](https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports) \[ This one has a text length about 3000 words and short summary, which is ideal \]

## Review

[Opinosis](http://kavita-ganesan.com/opinosis-opinion-dataset/#.YLu3Ky21FQI) \[ 51 data points \]

## Sentence

[Sentence-compressed](https://paperswithcode.com/dataset/sentence-compression) \[ Large corpus of uncompressed and compressed sentences from news articles \]


## Source

[nlp resources](https://github.com/mathsyouth/awesome-text-summarization)

[kaggle](https://www.kaggle.com)


[process the data](https://github.com/abisee/cnn-dailymail)

